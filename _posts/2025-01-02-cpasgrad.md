---
title: "cpasgrad: My nano c++ neural network framework."
date: 2025-01-02
categories: [c++, deep learning]
tags: [c++, deep learning]     # TAG names should always be lowercase
---

## Introduction   

There are two main objectives for this project:      
1. my knowledge of c++ is close to zero, so I want to improve my skills.     
2. build a deep learning framework myself with the knowledge I've acquired during my studies.      

In this context, this project is in no way intended to be optimized. It's just a way for me to improve my skills and make sure I understand my job.     

You can find the source code of the project here: [https://github.com/p-omahony/cpasgrad](https://github.com/p-omahony/cpasgrad).

## Problem formulation

Let's start by analyzing what we need to build a minimalist neural network framework: 
- a tensor object and his operations  
- an automatic differentiation engine       
- loss functions      
- optimization algorithms     
- neural network layers     
- metrics for evaluation

## What is a Tensor ?

A tensor, in deep learning, represents data of any arbitrary dimensionality. It can be thought of as an n-dimensional array or matrix that is used to store
numerical values, such as weights, biases, or activations in neural networks.          

Here are some key points about tensors:        

1. **Dimensionality**: Tensors can have any number of dimensions (n-dimensions). For example, and for this project I will focus on:               
   - A scalar (0-dimensional tensor) has no dimensions.     
   - A vector (1-dimensional tensor) has one dimension.      
   - A matrix (2-dimensional tensor) has two dimensions.       

2. **Size and Shape**: Each tensor has a specific size, which is defined by its shape. The shape of a tensor is a tuple that specifies the number of elements along each dimension.

3. **Data Types**: Tensors can store data of different types, such as floats (for real numbers), integers (for discrete values), or complex numbers.

4. **Operations**: Tensors support various operations, including addition, subtraction, multiplication, division, and more. These operations are performed element-wise across the
tensors.

5. **Use in Neural Networks**: In neural networks, tensors are used to store input data, weights, biases, activations, gradients, and other intermediate values during training and
inference.

### Tensor base class implementation

### Tensor operations implementation

## Automatic differentiation

## Loss functions

## Neural network layers

## Optimization algorithms

## Metrics

## Training

## Evaluation

## References

