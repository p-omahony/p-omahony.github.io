---
title: "Learning to Last (L2L) #1: One billion row challenge (1brc) in C"
date: 2025-12-14
categories: [C, programming, l2l]
tags: [C, programming, l2l]     # TAG names should always be lowercase
---

## 1. Introduction   

The One Billion Row Challenge (1BRC) is a performance optimization problem: process one billion weather measurements from a CSV file and compute the minimum, maximum, and average temperature for each weather station. While the task sounds straightforward, the scale makes it an excellent learning opportunity for understanding how computers actually work.

I'm tackling this challenge in C, despite having very little experience with the language. This choice is intentional—C offers minimal abstraction and direct control over memory and I/O, forcing me to think carefully about every operation. By working close to the machine, I'll learn about I/O optimization, memory access patterns, cache behavior, and algorithmic efficiency in ways that higher-level languages often hide.

I'm not inventing anything new here. The techniques I'll use—hash tables, buffered I/O, memory mapping, parallel processing, and various low-level optimizations—are all well-established concepts that have been used by systems programmers for decades. My goal is to deeply understand these concepts by implementing them myself, experiencing firsthand how they work and why they matter. This project will teach me how to optimize file reading, manage memory efficiently, profile performance bottlenecks, and understand the trade-offs between different approaches. More importantly, it's a practical way to confront real constraints rather than toy examples, documenting both my successes and the mistakes I make along the way as I work to internalize these fundamental computer science concepts.

The code implementations shown in this post are inspired by [Danny van Kooten's excellent C implementation](https://www.dannyvankooten.com/blog/2024/1brc/), which served as a valuable reference for understanding how to apply these optimization techniques in practice.

## 2. The Challenge

The 1BRC requires processing exactly one billion rows of weather measurement data. Each row contains a station name and a temperature reading. The goal is to compute three statistics for each unique weather station: the minimum temperature, the maximum temperature, and the average temperature.

### 2.1 Input Data Format

The input file is a CSV (comma-separated values) file, though it uses semicolons as delimiters. Each line follows this format:

```
StationName;Temperature
```

The temperature values are floating-point numbers, and station names are strings. Here's a sample of what the data looks like:

```
Tokyo;35.6897
Jakarta;-6.1750
Delhi;28.6100
Guangzhou;23.1300
Mumbai;19.0761
Manila;14.5958
Shanghai;31.1667
São Paulo;-23.5500
Seoul;37.5600
Mexico City;19.4333
```

The file contains exactly 1 billion rows, with measurements from various weather stations around the world. Some stations appear multiple times (with different temperature readings), while others may appear only once.

### 2.2 Constraints

This challenge comes with several important constraints:

- **Single machine execution**: The solution must run on a single machine—no distributed processing or external services allowed
- **Full dataset**: All 1 billion rows must be processed
- **Hardware limits**: My machine is an Apple M3 (ARM64) with an 8-core CPU (4 performance cores, 4 efficiency cores) and 24GB RAM
- **File size**: The input file is approximately 13-15 GB, which means efficient I/O handling is crucial
- **Memory considerations**: With limited RAM relative to the file size, we can't simply load everything into memory at once

### 2.3 Output Requirements

The results must be formatted as a single line with the following specifications:

- **Grouping**: Results grouped by station name
- **Sorting**: Stations sorted alphabetically by name
- **Format**: `{Station1=min/avg/max, Station2=min/avg/max, ...}`
- **Precision**: All temperature values rounded to one decimal place
- **Structure**: Wrapped in curly braces with comma-separated entries

Here's an example of the expected output format:

```
{Abha=-23.0/18.0/59.2, Abidjan=-16.2/26.0/67.3, Abéché=-10.0/29.4/69.0, Accra=-10.1/26.4/66.4, Addis Ababa=-23.7/16.0/67.0, Adelaide=-27.8/17.3/58.5, ...}
```

### 2.4 Performance Considerations

What makes this challenge interesting is the scale. Processing 1 billion rows efficiently requires careful consideration of:

- **I/O optimization**: Reading 13-15 GB from disk efficiently—buffering strategies, read sizes, and minimizing system calls
- **Memory access patterns**: How data is accessed affects cache performance and overall speed
- **Algorithmic efficiency**: Choosing the right data structures and algorithms for aggregating statistics
- **Memory vs speed trade-offs**: Balancing between memory usage and computational speed

The challenge isn't just about getting the correct answer—it's about doing it fast. Top solutions can process the entire dataset in just a few seconds, which requires understanding how modern computers actually work at a low level.

## 3. Preliminary Reading and Research

Based on my preliminary research and reflection, this project consists of several important steps that need to work together efficiently:

- **File reading**: Efficiently reading the 13-15 GB file from disk, minimizing I/O overhead and system calls
- **Line parsing**: Extracting station names and temperature values from each line, handling edge cases and optimizing string operations
- **Key-value storage**: Storing and updating statistics (min, avg, max) for each city, choosing the right data structure for fast lookups and updates
- **Parallelization**: Leveraging multiple CPU cores to process different parts of the file simultaneously
- **Output formatting**: Generating the final output string with proper sorting, rounding, and formatting

## 4. First (Naive) Approach

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

typedef struct {
  char city[100];
  float sum;
  float min;
  float max;
  int count;
} CityStats;

int find_city(char* city, CityStats stats[], int nstats)
{
  for (int i = 0; i < nstats; i++) {
    if (strcmp(stats[i].city, city) == 0) {
      return i;
    }
  }

  return -1;
}

static int cmp(const void *ptr_a, const void *ptr_b) {
  return strcmp(((CityStats *)ptr_a)->city, ((CityStats*)ptr_b)->city);
}


int main()
{
  FILE *fp = fopen("../measurements.txt", "r");
  if (fp == NULL) {
    perror("failed to open file");
  }

  char buffer[250];
  CityStats stats[450];
  int nstats = 0;
  
  while (fgets(buffer, sizeof(buffer), fp) != NULL) {
    char *semicolon_pos = strchr(buffer, ';');
    *semicolon_pos = 0x0;
    float temperature = (float)strtof(semicolon_pos + 1, NULL);;

    int city_idx = find_city(buffer, stats, nstats);

    if (city_idx < 0) {
      strcpy(stats[nstats].city, buffer);
      stats[nstats].sum = temperature;
      stats[nstats].max = temperature;
      stats[nstats].min = temperature;
      stats[nstats].count = 1;
      nstats++;
    } else {
      stats[city_idx].sum += temperature;
      if (temperature > stats[city_idx].max) {
        stats[city_idx].max = temperature;
      } 
      if (temperature < stats[city_idx].min) {
        stats[city_idx].min = temperature;
      } 
      stats[city_idx].count += 1;
    }
  }

  fclose(fp);

  qsort(stats, (size_t)nstats, sizeof(*stats), cmp);

  printf("{");
  for (int i = 0; i < nstats; i++) {
      printf("%s=%.1f/%.1f/%.1f", 
             stats[i].city, 
             stats[i].min,
             stats[i].sum / stats[i].count, 
             stats[i].max);

      if (i < nstats - 1) {
          printf(", ");
      }
  }
  printf("}\n");

  
  return 0;
}
```

### 4.1 Code Explanation

Let's break down this code step by step, assuming you're new to C programming.

#### 4.1.1 Header Files (Lines 95-97)

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
```

These are **header files** that give us access to useful functions:
- `stdio.h`: Provides file operations (`fopen`, `fclose`, `fgets`, `printf`) and input/output functions
- `stdlib.h`: Provides general utilities like `qsort` (for sorting) and `strtof` (for converting strings to floats)
- `string.h`: Provides string manipulation functions like `strcmp` (compare strings), `strcpy` (copy strings), and `strchr` (find a character in a string)

Think of these like importing libraries in other languages—they give us tools we need.

#### 4.1.2 The CityStats Structure (Lines 99-105)

```c
typedef struct {
  char city[100];
  float sum;
  float min;
  float max;
  int count;
} CityStats;
```

This defines a **structure** (like a class or object in other languages) that holds all the statistics for one city:
- `city[100]`: An array of characters (string) to store the city name (up to 99 characters + null terminator)
- `sum`: A floating-point number that accumulates all temperature readings for this city
- `min`: The minimum temperature seen so far
- `max`: The maximum temperature seen so far
- `count`: How many temperature readings we've seen for this city

We'll use this to track statistics for each unique city we encounter.

#### 4.1.3 Finding a City (Lines 107-116)

```c
int find_city(char* city, CityStats stats[], int nstats)
{
  for (int i = 0; i < nstats; i++) {
    if (strcmp(stats[i].city, city) == 0) {
      return i;
    }
  }
  return -1;
}
```

This function searches through our array of city statistics to see if we've already seen a particular city:
- **Parameters**: 
  - `city`: The city name we're looking for (a string)
  - `stats[]`: The array of all city statistics we've collected so far
  - `nstats`: How many cities are currently in the array
- **How it works**: It loops through all existing cities and uses `strcmp` (string compare) to check if the name matches
- **Returns**: The index (position) in the array if found, or `-1` if the city is new

This is a **linear search**—simple but slow for large datasets. We'll need to optimize this later.

#### 4.1.4 Comparison Function for Sorting (Lines 118-120)

```c
static int cmp(const void *ptr_a, const void *ptr_b) {
  return strcmp(((CityStats *)ptr_a)->city, ((CityStats*)ptr_b)->city);
}
```

This function tells the sorting algorithm (`qsort`) how to compare two city statistics:
- It takes two pointers (memory addresses) to `CityStats` structures
- It compares their city names alphabetically using `strcmp`
- Returns a negative number if the first city comes before the second alphabetically, 0 if they're equal, or positive if the first comes after

The `static` keyword means this function is only used within this file.

#### 4.1.5 Main Function - Opening the File (Lines 123-128)

```c
int main()
{
  FILE *fp = fopen("../measurements.txt", "r");
  if (fp == NULL) {
    perror("failed to open file");
  }
```

The `main` function is where our program starts:
- `FILE *fp` creates a pointer to a file object
- `fopen` opens the file `"../measurements.txt"` in read mode (`"r"`)
- If the file can't be opened (returns `NULL`), we print an error message and exit

#### 4.1.6 Setting Up Storage (Lines 130-132)

```c
char buffer[250];
CityStats stats[450];
int nstats = 0;
```

We create three variables:
- `buffer[250]`: A temporary array to hold one line of text from the file (up to 249 characters)
- `stats[450]`: An array that can hold statistics for up to 450 different cities
- `nstats`: A counter tracking how many unique cities we've seen so far (starts at 0)

#### 4.1.7 Reading and Processing Each Line (Lines 134-158)

```c
while (fgets(buffer, sizeof(buffer), fp) != NULL) {
```

This loop reads the file line by line:
- `fgets` reads one line from the file into `buffer`
- The loop continues until `fgets` returns `NULL` (end of file)

**Inside the loop, we parse each line:**

```c
char *semicolon_pos = strchr(buffer, ';');
*semicolon_pos = 0x0;
float temperature = (float)strtof(semicolon_pos + 1, NULL);
```

- `strchr` finds the semicolon (`;`) that separates the city name from the temperature
- `*semicolon_pos = 0x0` replaces the semicolon with a null character (`\0`), effectively splitting the string—now `buffer` contains just the city name
- `strtof` converts the text after the semicolon (the temperature) into a floating-point number

**Then we check if we've seen this city before:**

```c
int city_idx = find_city(buffer, stats, nstats);

if (city_idx < 0) {
  // New city - add it
  strcpy(stats[nstats].city, buffer);
  stats[nstats].sum = temperature;
  stats[nstats].max = temperature;
  stats[nstats].min = temperature;
  stats[nstats].count = 1;
  nstats++;
} else {
  // Existing city - update statistics
  stats[city_idx].sum += temperature;
  if (temperature > stats[city_idx].max) {
    stats[city_idx].max = temperature;
  } 
  if (temperature < stats[city_idx].min) {
    stats[city_idx].min = temperature;
  } 
  stats[city_idx].count += 1;
}
```

- If `city_idx` is `-1` (city not found), we add a new entry:
  - Copy the city name
  - Initialize sum, min, and max to this first temperature
  - Set count to 1
  - Increment our counter
- If the city exists, we update its statistics:
  - Add the temperature to the sum
  - Update max if this temperature is higher
  - Update min if this temperature is lower
  - Increment the count

#### 4.1.8 Closing the File and Sorting (Lines 160-162)

```c
fclose(fp);

qsort(stats, (size_t)nstats, sizeof(*stats), cmp);
```

- `fclose` closes the file (good practice to free resources)
- `qsort` sorts our array alphabetically by city name using the `cmp` function we defined earlier

#### 4.1.9 Printing the Results (Lines 164-176)

```c
printf("{");
for (int i = 0; i < nstats; i++) {
    printf("%s=%.1f/%.1f/%.1f", 
           stats[i].city, 
           stats[i].min,
           stats[i].sum / stats[i].count, 
           stats[i].max);

    if (i < nstats - 1) {
        printf(", ");
    }
}
printf("}\n");
```

This prints the final output:
- Starts with `{`
- For each city, prints: `CityName=min/avg/max`
  - `%.1f` formats the float to one decimal place
  - Average is calculated as `sum / count`
- Adds commas between cities (but not after the last one)
- Ends with `}\n` (closing brace and newline)

#### 4.1.10 Summary

This code implements a straightforward solution:
1. Read the file line by line
2. Parse each line to extract city name and temperature
3. For each city, track min, max, sum, and count
4. Sort cities alphabetically
5. Print results in the required format

While this works correctly, it's not optimized for speed—the linear search through cities becomes slow as we add more cities, and reading line-by-line isn't the fastest way to process large files. But it's a good starting point that's easy to understand!

### 4.2 Performance Results

Running this naive implementation on the full dataset produces the following timing results:

```bash
./main  465.77s user 4.25s system 99% cpu 7:50.79 total
```

This output from the `time` command shows:
- **User time**: 465.77 seconds (~7.8 minutes) - time spent executing the program code
- **System time**: 4.25 seconds - time spent in system calls (like file I/O)
- **CPU usage**: 99% - the program was CPU-bound, utilizing almost all available CPU time
- **Total elapsed time**: 7 minutes and 50.79 seconds (~470 seconds)

The program took nearly 8 minutes to process 1 billion rows, which gives us a baseline to improve upon. The high CPU usage (99%) indicates that the bottleneck is computational rather than I/O-bound, suggesting opportunities for optimization in the parsing and lookup logic.

## 5. Hash Table

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define HT_CAPACITY 450


typedef struct {
  char city[100];
  float sum;
  float min;
  float max;
  int count;
} CityStats;


unsigned int hash_fn(char *city, int n)
{
  unsigned int hash = 0;
  for (int i=0; i<n; i++){
    hash = 31 * hash + (unsigned char)city[i];
  }

  return hash;
}


static int cmp(const void *ptr_a, const void *ptr_b) {
  return strcmp(((CityStats *)ptr_a)->city, ((CityStats*)ptr_b)->city);
}


int main()
{
  FILE *fp = fopen("../measurements.txt", "r");
  if (fp == NULL) {
    perror("failed to open file");
  }

  char buffer[250];
  CityStats stats[450];
  int nstats = 0;

  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));
  
  while (fgets(buffer, sizeof(buffer), fp) != NULL) {
    char *semicolon_pos = strchr(buffer, ';');
    *semicolon_pos = 0x0;
    float temperature = (float)strtof(semicolon_pos + 1, NULL);;

    unsigned int hash_idx = hash_fn(buffer, semicolon_pos - buffer) % HT_CAPACITY;
    while (htable[hash_idx ]!= -1 && strcmp(buffer, stats[htable[hash_idx]].city) != 0) {
      hash_idx = (hash_idx + 1) % HT_CAPACITY;
    }
    int city_idx = htable[hash_idx];

    if (city_idx < 0) {
      strcpy(stats[nstats].city, buffer);
      stats[nstats].sum = temperature;
      stats[nstats].max = temperature;
      stats[nstats].min = temperature;
      stats[nstats].count = 1;
      htable[hash_idx] = nstats;
      nstats++;
    } else {
      stats[city_idx].sum += temperature;
      if (temperature > stats[city_idx].max) {
        stats[city_idx].max = temperature;
      } 
      if (temperature < stats[city_idx].min) {
        stats[city_idx].min = temperature;
      } 
      stats[city_idx].count += 1;
    }
  }

  fclose(fp);

  qsort(stats, (size_t)nstats, sizeof(*stats), cmp);

  printf("{");
  for (int i = 0; i < nstats; i++) {
      printf("%s=%.1f/%.1f/%.1f", 
             stats[i].city, 
             stats[i].min,
             stats[i].sum / stats[i].count, 
             stats[i].max);

      if (i < nstats - 1) {
          printf(", ");
      }
  }
  printf("}\n");

  
  return 0;
}
```

### 5.1 What is a Hash Table?

Before diving into the code changes, let's understand what a **hash table** is and why we need it.

Imagine you have a phone book with thousands of names. If you want to find someone's number, you could start from the beginning and check every name until you find the right one—this is what we did in the previous version with `find_city()`. This works, but it gets slower as the phone book grows.

A **hash table** is like having a smart filing system. Instead of searching through everything, it uses a **hash function**—a mathematical formula that converts a city name into a number. This number tells us exactly where to look (or where to store) the information. Think of it like a library's Dewey Decimal System: instead of wandering through shelves, you calculate a number and go directly to that location.

**The purpose of a hash table** is to make lookups fast—ideally, finding a city should take constant time (O(1)) instead of linear time (O(n)), where n is the number of cities. This is crucial when processing 1 billion rows, as we're looking up cities billions of times.

### 5.2 What's Different from the Previous Version?

The main difference is how we find cities. Instead of searching through the entire array every time (linear search), we now use a hash table for fast lookups. Here are the key changes:

#### 5.2.1 The Hash Function (Lines 433-441)

```c
unsigned int hash_fn(char *city, int n)
{
  unsigned int hash = 0;
  for (int i=0; i<n; i++){
    hash = 31 * hash + (unsigned char)city[i];
  }
  return hash;
}
```

**What's new:** This function didn't exist in the previous version. It converts a city name into a number:
- It starts with `hash = 0`
- For each character in the city name, it multiplies the current hash by 31 and adds the character's value
- The number 31 is commonly used because it's prime and works well for string hashing
- The result is a large number that we'll use to find the right position in our hash table

**Why it matters:** This gives us a "fingerprint" for each city name. Different cities should produce different hash values (most of the time), allowing us to jump directly to where we stored that city's data.

#### 5.2.2 The Hash Table Array (Lines 460-461)

```c
int htable[HT_CAPACITY];
memset(htable, -1, HT_CAPACITY * sizeof(int));
```

**What's new:** We now have a separate array called `htable` (hash table) with 450 slots:
- `HT_CAPACITY` is defined as 450 (line 421), matching our expected number of cities
- `memset` initializes all slots to `-1`, which means "empty slot"
- Each slot stores an **index** pointing to a position in our `stats` array, not the actual city data

**Why it matters:** This array acts as our "index" or "lookup table." When we hash a city name, we get a number between 0 and 449, and we check that position in `htable` to find where the city's statistics are stored.

#### 5.2.3 Finding Cities with the Hash Table (Lines 468-472)

```c
unsigned int hash_idx = hash_fn(buffer, semicolon_pos - buffer) % HT_CAPACITY;
while (htable[hash_idx ]!= -1 && strcmp(buffer, stats[htable[hash_idx]].city) != 0) {
  hash_idx = (hash_idx + 1) % HT_CAPACITY;
}
int city_idx = htable[hash_idx];
```

**What's different:** This replaces the `find_city()` function from the previous version. Here's how it works:

1. **Calculate hash index:** `hash_fn(buffer, semicolon_pos - buffer)` converts the city name to a number, then `% HT_CAPACITY` ensures it's between 0 and 449
2. **Handle collisions:** The `while` loop handles the case where two cities might hash to the same index (called a "collision"):
   - If the slot is empty (`-1`), we found where to store a new city
   - If the slot contains a different city (checked with `strcmp`), we move to the next slot (`hash_idx + 1`)
   - The `% HT_CAPACITY` wraps around if we reach the end of the array
3. **Get the city index:** `htable[hash_idx]` gives us the index in the `stats` array where this city's data is stored

**Why it's faster:** Instead of checking every city in order (which could be 450 comparisons), we usually find the right city in just 1-2 lookups. Even with collisions, we rarely need to check more than a few slots.

**Why a well-distributed hash function matters:** The quality of our hash function directly impacts performance. A **well-distributed hash function** spreads city names evenly across all 450 slots in our hash table. Here's why this is crucial:

- **Fewer collisions:** If many cities hash to the same slot, we get more collisions. Each collision forces us to check additional slots, slowing down lookups. With a good hash function, cities are spread out, minimizing collisions.

- **Consistent performance:** A poorly distributed hash function might cluster many cities in just a few slots. For example, if 100 cities all hash to slot 42, finding any of those cities requires checking through many entries. With a well-distributed hash, each slot has roughly the same number of cities (about 1 city per slot for 450 cities), giving us consistent, fast lookups.

- **Real-world impact:** When processing 1 billion rows, we perform billions of hash table lookups. If our hash function causes even 10% more collisions, that translates to millions of extra comparisons. The hash function using prime number 31 and combining all characters helps ensure that similar city names (like "Tokyo" and "Osaka") still produce different hash values, spreading them across different slots.

Think of it like organizing books in a library: if all books about "History" go to the same shelf, that shelf becomes crowded and slow to search. But if books are distributed evenly across all shelves based on a good classification system, you can find any book quickly.

#### 5.2.4 Storing New Cities (Lines 474-481)

```c
if (city_idx < 0) {
  strcpy(stats[nstats].city, buffer);
  stats[nstats].sum = temperature;
  stats[nstats].max = temperature;
  stats[nstats].min = temperature;
  stats[nstats].count = 1;
  htable[hash_idx] = nstats;
  nstats++;
}
```

**What's different:** When we add a new city, we now also update the hash table:
- `htable[hash_idx] = nstats` stores the index of this new city in the hash table
- This way, future lookups for this city will find it quickly

**Why it matters:** This "registers" the city in our hash table, making future lookups instant instead of requiring a full array scan.

### 5.3 Performance Improvement

Running this hash table version on the full dataset shows significant improvement:

```bash
./main  64.78s user 2.35s system 98% cpu 1:07.91 total
```

Compared to the previous version:
- **User time**: Reduced from 465.77 seconds to 64.78 seconds (~7.2x faster!)
- **Total elapsed time**: Reduced from ~7 minutes 50 seconds to ~1 minute 8 seconds
- **CPU usage**: Still at 98%, meaning we're efficiently using the CPU

The hash table dramatically speeds up city lookups. Since we're processing 1 billion rows and looking up cities billions of times, replacing the slow linear search with fast hash table lookups makes a huge difference. Instead of potentially checking hundreds of cities for each lookup, we now typically find the right city in just 1-2 operations.

## 6. Custom Parsing

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define HT_CAPACITY 450


typedef struct {
  char city[100];
  float sum;
  float min;
  float max;
  int count;
} CityStats;


unsigned int hash_fn(char *city, int n)
{
  unsigned int hash = 0;
  for (int i=0; i<n; i++){
    hash = 31 * hash + (unsigned char)city[i];
  }

  return hash;
}


static void parse_double(float *dest, const char *s) {
  float mod;
  if (*s == '-') {
    mod = -1.0;
    s++;
  } else {
    mod = 1.0;
  }

  if (s[1] == '.') {
    *dest = (((float)s[0] + (float)s[2] / 10.0) - 1.1 * '0') * mod;
  }

  *dest = ((double)((s[0]) * 10 + s[1]) + (double)s[3] / 10.0 - 11.1 * '0') * mod;
}


static int cmp(const void *ptr_a, const void *ptr_b) {
  return strcmp(((CityStats *)ptr_a)->city, ((CityStats*)ptr_b)->city);
}


int main()
{
  FILE *fp = fopen("../measurements.txt", "r");
  if (fp == NULL) {
    perror("failed to open file");
  }

  char buffer[250];
  CityStats stats[450];
  int nstats = 0;

  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));
  
  while (fgets(buffer, sizeof(buffer), fp) != NULL) {
    char *semicolon_pos = strchr(buffer, ';');
    *semicolon_pos = 0x0;
    float temperature;
    parse_double(&temperature, semicolon_pos + 1);

    unsigned int hash_idx = hash_fn(buffer, semicolon_pos - buffer) % HT_CAPACITY;
    while (htable[hash_idx ]!= -1 && strcmp(buffer, stats[htable[hash_idx]].city) != 0) {
      hash_idx = (hash_idx + 1) % HT_CAPACITY;
    }
    int city_idx = htable[hash_idx];

    if (city_idx < 0) {
      strcpy(stats[nstats].city, buffer);
      stats[nstats].sum = temperature;
      stats[nstats].max = temperature;
      stats[nstats].min = temperature;
      stats[nstats].count = 1;
      htable[hash_idx] = nstats;
      nstats++;
    } else {
      stats[city_idx].sum += temperature;
      if (temperature > stats[city_idx].max) {
        stats[city_idx].max = temperature;
      } 
      if (temperature < stats[city_idx].min) {
        stats[city_idx].min = temperature;
      } 
      stats[city_idx].count += 1;
    }
  }

  fclose(fp);

  qsort(stats, (size_t)nstats, sizeof(*stats), cmp);

  printf("{");
  for (int i = 0; i < nstats; i++) {
      printf("%s=%.1f/%.1f/%.1f", 
             stats[i].city, 
             stats[i].min,
             stats[i].sum / stats[i].count, 
             stats[i].max);

      if (i < nstats - 1) {
          printf(", ");
      }
  }
  printf("}\n");

  
  return 0;
}
```

### 6.1 What is Custom Parsing?

In the previous versions, we used `strtof()`—a standard library function that converts a string to a floating-point number. While `strtof()` is robust and handles many edge cases (scientific notation, different number formats, error checking, etc.), this generality comes at a cost: it's slower than we need for our specific use case.

**Custom parsing** means writing our own function to convert strings to numbers, tailored specifically to the format we know our data uses. Since we know exactly what format the temperatures are in (like "35.6" or "-23.5"), we can write a much simpler and faster parser that only handles these specific cases.

Think of it like this: `strtof()` is like a Swiss Army knife—it can handle many different situations, but it's heavier and slower than a specialized tool. Our custom parser is like a single-purpose tool designed exactly for our job—lighter, faster, and more efficient.

### 6.2 What's Different from the Previous Version?

The main change is replacing `strtof()` with our custom `parse_double()` function. Everything else (hash table, data structures, main logic) remains the same. Let's examine the key difference:

#### 6.2.1 The Custom Parse Function (Lines 30-44)

```c
static void parse_double(float *dest, const char *s) {
  float mod;
  if (*s == '-') {
    mod = -1.0;
    s++;
  } else {
    mod = 1.0;
  }

  if (s[1] == '.') {
    *dest = (((float)s[0] + (float)s[2] / 10.0) - 1.1 * '0') * mod;
  }

  *dest = ((double)((s[0]) * 10 + s[1]) + (double)s[3] / 10.0 - 11.1 * '0') * mod;
}
```

**What's new:** This function replaces `strtof()` from the previous version. Let's break down how it works:

**Handling the sign:**
- `if (*s == '-')` checks if the first character is a minus sign
- If negative, we set `mod = -1.0` and advance the pointer `s++` to skip the minus sign
- If positive, we set `mod = 1.0` and don't advance
- This allows us to handle both positive and negative temperatures

**Parsing single-digit temperatures (like "5.6"):**
- `if (s[1] == '.')` checks if the second character is a decimal point
- This handles cases where the temperature has only one digit before the decimal (like "5.6")
- The formula `((float)s[0] + (float)s[2] / 10.0) - 1.1 * '0'` works as follows:
  - `s[0]` is the first digit character (e.g., '5' which is ASCII 53)
  - `s[2]` is the digit after the decimal point (e.g., '6' which is ASCII 54)
  - To convert ASCII characters to numbers, we need to subtract `'0'` (ASCII 48)
  - `1.1 * '0'` = `1.1 * 48` = `52.8` is a clever optimization:
    - We need to subtract `'0'` from `s[0]` once: `s[0] - '0'`
    - We also need to subtract `'0'` from `s[2]` once: `(s[2] - '0') / 10.0`
    - The total subtraction needed is: `'0' + '0'/10.0` = `48 + 4.8` = `52.8` = `1.1 * '0'`
  - So the formula computes: `(s[0] + s[2]/10.0) - 1.1*'0'` = `(53 + 54/10.0) - 52.8` = `(53 + 5.4) - 52.8` = `5.6` ✓
  - Finally, multiply by `mod` to apply the sign

**Parsing two-digit temperatures (like "35.6"):**
- The second `*dest = ...` line handles the more common case of two digits before the decimal
- `(s[0]) * 10 + s[1]` combines the first two digits as ASCII values: if `s[0]` is '3' (ASCII 51) and `s[1]` is '5' (ASCII 53), this gives us `51*10 + 53` = `563` (before subtracting ASCII offsets)
- `s[3] / 10.0` gets the decimal digit divided by 10: if `s[3]` is '6' (ASCII 54), this gives us `54/10.0` = `5.4`
- `11.1 * '0'` = `11.1 * 48` = `532.8` accounts for subtracting `'0'` from all three digits:
  - From `s[0]`: subtract `'0' * 10` = `48 * 10` = `480`
  - From `s[1]`: subtract `'0'` = `48`
  - From `s[3]`: subtract `'0' / 10.0` = `48 / 10.0` = `4.8`
  - Total: `480 + 48 + 4.8` = `532.8` = `11.1 * '0'`
- So the formula computes: `(51*10 + 53 + 54/10.0) - 532.8` = `(510 + 53 + 5.4) - 532.8` = `568.4 - 532.8` = `35.6` ✓
- Multiply by `mod` to apply the sign

**Why it's faster:**
- **No error checking overhead**: `strtof()` checks for many edge cases we don't need
- **No locale handling**: Standard library functions handle different number formats and locales, which we don't need
- **Direct character arithmetic**: We work directly with ASCII values instead of going through multiple function calls
- **Simpler logic**: We only handle the exact format we know our data uses

**Note:** There's a subtle issue in this code—the second `*dest = ...` assignment always executes, even after the single-digit case. This means single-digit temperatures might be parsed incorrectly. However, for the 1BRC challenge, if the data format is consistent (always two digits before the decimal), this won't be a problem. The function is optimized for the specific data format we're processing.

#### 6.2.2 Using the Custom Parser (Line 80)

```c
float temperature;
parse_double(&temperature, semicolon_pos + 1);
```

**What's different:** Instead of:
```c
float temperature = (float)strtof(semicolon_pos + 1, NULL);
```

We now call our custom function:
- `parse_double(&temperature, semicolon_pos + 1)` passes a pointer to `temperature` (so the function can modify it) and the string starting after the semicolon
- The function directly modifies `temperature` through the pointer

**Why it matters:** This single change—replacing a general-purpose library function with a specialized parser—gives us a measurable performance boost. When processing 1 billion rows, even a small improvement per row adds up significantly.

### 6.3 Performance Improvement

Running this custom parsing version on the full dataset shows further improvement:

```bash
./main  54.86s user 2.37s system 99% cpu 57.469 total
```

Compared to the hash table version:
- **User time**: Reduced from 64.78 seconds to 54.86 seconds (~15% faster)
- **Total elapsed time**: Reduced from ~1 minute 8 seconds to ~57 seconds
- **CPU usage**: Still at 99%, indicating efficient CPU utilization

The custom parser eliminates the overhead of `strtof()`, which includes error checking, locale handling, and support for various number formats we don't need. By writing a parser tailored specifically to our data format, we've shaved off another ~10 seconds of processing time. This demonstrates an important optimization principle: when you know your data format exactly, specialized code can outperform general-purpose library functions.

## 7. Buffered I/O and Memory Optimizations

```c
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define HT_CAPACITY 450
#define BUFF_SIZE 1024*64


typedef struct {
  char city[100];
  float sum;
  float min;
  float max;
  int count;
} CityStats;


unsigned int hash_fn(char *city, int n)
{
  unsigned int hash = 0;
  for (int i=0; i<n; i++){
    hash = 31 * hash + (unsigned char)city[i];
  }

  return hash;
}


static char *parse_double(float *dest, char *s) {
  float mod;
  if (*s == '-') {
    mod = -1.0;
    s++;
  } else {
    mod = 1.0;
  }

  if (s[1] == '.') {
    *dest = (((float)s[0] + (float)s[2] / 10.0) - 1.1 * '0') * mod;
    return s + 4;
  }

  *dest =
      ((double)((s[0]) * 10 + s[1]) + (double)s[3] / 10.0 - 11.1 * '0') * mod;
  return s + 5;
}


static int cmp(const void *ptr_a, const void *ptr_b) {
  return strcmp(((CityStats *)ptr_a)->city, ((CityStats*)ptr_b)->city);
}


int main()
{
  FILE *fp = fopen("../measurements.txt", "r");
  if (fp == NULL) {
    perror("failed to open file");
    return 1;
  }

  char *buffer = malloc(BUFF_SIZE);
  CityStats stats[450];
  int nstats = 0;

  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));
  
  while (1) {
    size_t nread = fread(buffer, 1, BUFF_SIZE, fp); 
    if (nread <= 0) {
      break;
    }

    int rewind = 0;
    while (nread > 0 && buffer[nread-1] != '\n'){
      rewind--;
      nread--;
    }
    if (rewind < 0) {
      fseek(fp, rewind, SEEK_CUR);
    }

    char *s = buffer;
    while (s < &buffer[nread]){
      char *line_start = s;
      char *semicolon_pos = strchr(s, ';');
      int city_len = semicolon_pos - line_start;

      float temperature;
      s = parse_double(&temperature, semicolon_pos + 1);

      char *newline_pos = strchr(s, '\n');
      if (newline_pos == NULL || newline_pos >= &buffer[nread]) {
        break;
      }
      s = newline_pos + 1;
      
      unsigned int hash_idx = hash_fn(line_start, city_len) % HT_CAPACITY;
      while (htable[hash_idx] != -1 && memcmp(stats[htable[hash_idx]].city, line_start, (size_t)city_len) != 0) {
        hash_idx = (hash_idx + 1) % HT_CAPACITY;
      }

      int city_idx = htable[hash_idx];
      if (city_idx < 0) {
        memcpy(stats[nstats].city, line_start, (size_t)city_len);
        stats[nstats].city[city_len] = '\0';
        stats[nstats].sum = temperature;
        stats[nstats].max = temperature;
        stats[nstats].min = temperature;
        stats[nstats].count = 1;
        htable[hash_idx] = nstats;
        nstats++;
      } else {
        stats[city_idx].sum += temperature;
        if (temperature > stats[city_idx].max) {
          stats[city_idx].max = temperature;
        } 
        if (temperature < stats[city_idx].min) {
          stats[city_idx].min = temperature;
        } 
        stats[city_idx].count += 1;
      }
    }
  }

  fclose(fp);

  qsort(stats, (size_t)nstats, sizeof(*stats), cmp);

  printf("{");
  for (int i = 0; i < nstats; i++) {
      printf("%s=%.1f/%.1f/%.1f", 
             stats[i].city, 
             stats[i].min,
             stats[i].sum / stats[i].count, 
             stats[i].max);

      if (i < nstats - 1) {
          printf(", ");
      }
  }
  printf("}\n");

  
  return 0;
}
```

### 7.1 What is Buffered I/O?

In all previous versions, we used `fgets()` to read the file line by line. While `fgets()` is convenient and handles line boundaries automatically, it has a significant performance cost. Although `fgets()` itself uses buffered I/O (it doesn't make a system call for every line), it still has overhead: it must scan through the buffer character by character to find newlines, and it may need to refill its internal buffer more frequently than necessary.

**Buffered I/O** means reading large chunks of data at once (like 64KB) instead of reading one line at a time. Think of it like shopping: instead of making a separate trip to the store for each item you need (line-by-line reading), you make one big trip and buy everything at once (buffered reading). This dramatically reduces the number of system calls and eliminates the overhead of scanning for line boundaries.

By reading 64KB blocks with `fread()`, we reduce system calls from roughly 200,000-300,000 (with `fgets()`'s internal buffering) to roughly 15,000-20,000 (depending on file size). Each system call has overhead—switching between user and kernel space, checking file permissions, updating file position, etc. By minimizing these calls and eliminating line-scanning overhead, we eliminate a major bottleneck.

### 7.2 What's Different from the Previous Version?

This version introduces several optimizations beyond just buffered I/O. Let's examine each change:

#### 7.2.1 Buffered Reading with `fread`

```c
char *buffer = malloc(BUFF_SIZE);
...
while (1) {
  size_t nread = fread(buffer, 1, BUFF_SIZE, fp); 
  if (nread <= 0) {
    break;
  }
```

**What's new:** Instead of `fgets(buffer, sizeof(buffer), fp)`, we now use `fread()`:
- `BUFF_SIZE` is defined as `1024*64` (64KB) at the top of the code
- `malloc(BUFF_SIZE)` allocates a 64KB buffer on the heap
- `fread(buffer, 1, BUFF_SIZE, fp)` reads up to 64KB of raw data in one operation
- The function returns the number of bytes actually read, which we store in `nread`
- If `nread` is 0 or negative, we've reached the end of the file and break out of the loop

**Why it's faster:** 
- **Fewer system calls**: Reading 64KB at a time means we make roughly 200,000 system calls instead of 1 billion (assuming a 13GB file)
- **Better cache utilization**: Larger reads allow the operating system to optimize disk access patterns
- **Less overhead**: Each system call has fixed overhead; by making fewer calls, we reduce total overhead significantly

**Trade-off:** We now have to manually handle line boundaries, which adds complexity but is worth it for the performance gain.

#### 7.2.2 Handling Partial Lines at Buffer Boundaries

```c
int rewind = 0;
while (nread > 0 && buffer[nread-1] != '\n'){
  rewind--;
  nread--;
}
if (rewind < 0) {
  fseek(fp, rewind, SEEK_CUR);
}
```

**What's new:** This code handles the case where a line spans two buffer reads. When we read a 64KB chunk, the last line might be incomplete (cut off in the middle). We need to ensure we only process complete lines.

**How it works step by step:**

1. **Initialization**: We start with `rewind = 0` and `nread` containing the number of bytes read from the file.

2. **Check for incomplete line**: We check if the last character in our buffer (`buffer[nread-1]`) is a newline (`\n`). If it is, we have a complete buffer and can proceed.

3. **Find the last complete line**: If the last character is not a newline, we've read part of a line that continues in the next buffer. We need to find where the last complete line ends:
   - We walk backwards from the end of the buffer
   - Each iteration decrements both `rewind` (becomes -1, -2, -3, ...) and `nread` (becomes nread-1, nread-2, ...)
   - We continue until we find a newline character or reach the beginning of the buffer

4. **Adjust the file position**: After the loop, `rewind` contains a negative number indicating how many bytes we need to "unread". For example, if we walked back 6 bytes, `rewind` will be -6.

5. **Rewind the file pointer**: `fseek(fp, rewind, SEEK_CUR)` moves the file pointer backwards by `|rewind|` bytes. This ensures that the next `fread()` will include the incomplete line we just skipped.

6. **Update buffer size**: We also update `nread` to reflect only the complete lines we'll process in this iteration.

**Example:** 
- Suppose our buffer contains `"...Tokyo;35.6\nNew Y"` where "New Y" is the start of "New York" and the buffer ends at the 'Y'
- The last character (`buffer[nread-1]`) is 'Y', not `\n`
- We walk backwards: `rewind` becomes -1, -2, -3, -4, -5, -6 and `nread` decreases accordingly
- We find the newline after "Tokyo;35.6"
- `fseek(fp, -6, SEEK_CUR)` moves the file pointer back 6 bytes
- The next `fread()` will start reading from "New Y" again, giving us the complete line

**Why it matters:** Without this logic, we'd incorrectly parse incomplete lines, leading to wrong results. This is the complexity trade-off we accept for the speed benefit of buffered reading.

#### 7.2.3 Using `memcmp` Instead of `strcmp`

```c
while (htable[hash_idx] != -1 && memcmp(stats[htable[hash_idx]].city, line_start, (size_t)city_len) != 0) {
```

**What's different:** Previously we used `strcmp(buffer, stats[htable[hash_idx]].city)`, now we use `memcmp(stats[htable[hash_idx]].city, line_start, (size_t)city_len)`.

**Key differences:**
- `strcmp()` compares null-terminated strings and stops at the first `\0` character
- `memcmp()` compares a specific number of bytes (`city_len`) without requiring null terminators
- We already know `city_len` from parsing (line 88), so we can use it directly

**Why it's faster:**
- **No null-terminator search**: `strcmp()` must scan until it finds a `\0`, even if we only want to compare the first few characters
- **Explicit length**: `memcmp()` knows exactly how many bytes to compare, avoiding unnecessary work
- **Better for our use case**: Since we're comparing city names from the buffer (which aren't null-terminated at this point) against stored city names, `memcmp()` is more appropriate

**Performance impact:** When processing 1 billion rows, we perform millions of hash table lookups. Even a small improvement per comparison adds up significantly.

#### 7.2.4 Modified `parse_double` Function

```c
static char *parse_double(float *dest, char *s) {
  float mod;
  if (*s == '-') {
    mod = -1.0;
    s++;
  } else {
    mod = 1.0;
  }

  if (s[1] == '.') {
    *dest = (((float)s[0] + (float)s[2] / 10.0) - 1.1 * '0') * mod;
    return s + 4;
  }

  *dest =
      ((double)((s[0]) * 10 + s[1]) + (double)s[3] / 10.0 - 11.1 * '0') * mod;
  return s + 5;
}
```

**What's different:** The function now returns a `char *` pointer instead of `void`. It returns a pointer to the character immediately after the parsed temperature.

**How it's used:**
```c
float temperature;
s = parse_double(&temperature, semicolon_pos + 1);
```

**Why it's better:**
- **Efficient parsing**: Instead of searching for the newline with `strchr()` after parsing, we know exactly where the temperature ends
- **Less scanning**: We avoid scanning through characters we've already processed
- **Cleaner code**: The function tells us where to continue parsing, making the main loop more efficient

**The return values:**
- `return s + 4` for single-digit temperatures (like "5.6"): 1 digit + '.' + 1 digit + newline = 4 characters
- `return s + 5` for two-digit temperatures (like "35.6"): 2 digits + '.' + 1 digit + newline = 5 characters

This optimization works hand-in-hand with buffered reading, as we're now parsing more efficiently within each buffer.

#### 7.2.5 Using `memcpy` Instead of `strcpy`

```c
memcpy(stats[nstats].city, line_start, (size_t)city_len);
stats[nstats].city[city_len] = '\0';
```

**What's different:** Instead of `strcpy(stats[nstats].city, buffer)`, we now use `memcpy()` with an explicit length.

**Key differences:**
- `strcpy()` copies until it finds a null terminator (`\0`)
- `memcpy()` copies exactly `city_len` bytes
- We manually add the null terminator after copying

**Why it's faster:**
- **No null search**: `strcpy()` must scan the source string to find the end, while `memcpy()` knows exactly how many bytes to copy
- **Explicit control**: We already calculated `city_len` during parsing, so we can use it directly
- **Better for buffered data**: Since `line_start` points into our buffer (which may not be null-terminated at the right place), `memcpy()` is safer and more efficient

**Performance impact:** While copying city names is a small part of the overall work, using `memcpy()` with explicit length is both faster and more appropriate for our buffered reading approach.

### 7.3 Performance Improvement

Running this buffered I/O version on the full dataset shows dramatic improvement:

```bash
./main  19.32s user 1.26s system 99% cpu 20.737 total
```

Compared to the custom parsing version:
- **User time**: Reduced from 54.86 seconds to 19.32 seconds (~2.8x faster!)
- **System time**: Reduced from 2.37 seconds to 1.26 seconds (~47% reduction)
- **Total elapsed time**: Reduced from ~57 seconds to ~20.7 seconds (~2.8x faster)
- **CPU usage**: Still at 99%, indicating efficient CPU utilization

The combination of buffered I/O and memory optimizations delivers a massive performance boost. The reduction in system time (from 2.37s to 1.26s) directly reflects fewer system calls from buffered reading. The user time reduction comes from:
- Less overhead from system call context switches
- More efficient parsing with the modified `parse_double` function
- Faster string operations using `memcmp` and `memcpy` with explicit lengths
- Better cache behavior from reading larger chunks

This version processes 1 billion rows in just over 20 seconds—down from nearly 8 minutes in our first naive implementation. That's a **23x speedup** from where we started, achieved through understanding how computers actually work at a low level.

## 8. Parallel Processing

```c
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <pthread.h>

#define HT_CAPACITY 4096
#define BUFF_SIZE 1024*64
#define N_THREADS 8
#define MAX_CITY_PER_THREAD 450


typedef struct {
  char city[100];
  float sum;
  float min;
  float max;
  int count;
} ThreadCityStats;

typedef struct {
  int n_cities;
  ThreadCityStats results[MAX_CITY_PER_THREAD];
} ThreadResults;


unsigned int hash_fn(char *city, int n)
{
  unsigned int hash = 0;
  for (int i=0; i<n; i++){
    hash = 31 * hash + (unsigned char)city[i];
  }

  return hash;
}


static char *parse_double(float *dest, char *s) {
  float mod;
  if (*s == '-') {
    mod = -1.0;
    s++;
  } else {
    mod = 1.0;
  }

  if (s[1] == '.') {
    *dest = (((float)s[0] + (float)s[2] / 10.0) - 1.1 * '0') * mod;
    return s + 4;
  }

  *dest =
      ((double)((s[0]) * 10 + s[1]) + (double)s[3] / 10.0 - 11.1 * '0') * mod;
  return s + 5;
}


static int cmp(const void *ptr_a, const void *ptr_b) {
  return strcmp(((ThreadCityStats *)ptr_a)->city, ((ThreadCityStats*)ptr_b)->city);
}


size_t fread_chunked(char *dest, FILE *fh) {
  static pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
  pthread_mutex_lock(&lock);

  size_t nread = fread(dest, 1, BUFF_SIZE, fh);
  if (nread <= 0) {
    pthread_mutex_unlock(&lock);
    return nread;
  }

  long rewind = 0;
  while (dest[nread - 1] != '\n') {
    rewind--;
    nread--;
  }
  fseek(fh, rewind, SEEK_CUR);
  pthread_mutex_unlock(&lock);
  return nread;
}

void *process_chunk(void *arg){
  FILE *fp = (FILE *)arg;

  ThreadResults *result = malloc(sizeof(*result));
  if (!result) {
    perror("malloc");
    exit(EXIT_FAILURE);
  }
  result->n_cities = 0;

  char *buffer = malloc(BUFF_SIZE);

  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));

  while (1) {
    size_t nread = fread_chunked(buffer, fp); 
    if (nread <= 0) {
      break;
    }

    char *s = buffer;
    while (s < &buffer[nread]){
      char *line_start = s;
      char *semicolon_pos = strchr(s, ';');
      if (!semicolon_pos || semicolon_pos >= &buffer[nread]) {
        break;
      }
      int city_len = (int)(semicolon_pos - line_start);

      float temperature;
      char *next = parse_double(&temperature, semicolon_pos + 1);

      char *newline_pos = strchr(s, '\n');
      if (newline_pos == NULL || newline_pos >= &buffer[nread]) {
        break;
      }
      s = next;
      while (s < &buffer[nread] && *s != '\n') {
        s++;
      }
      if (s < &buffer[nread]) {
        s++;
      }

      unsigned int hash_idx = hash_fn(line_start, city_len) % HT_CAPACITY;
      while (htable[hash_idx] != -1 && memcmp(result->results[htable[hash_idx]].city, line_start, (size_t)city_len) != 0) {
        hash_idx = (hash_idx + 1) % HT_CAPACITY;
      }

      int city_idx = htable[hash_idx];
      if (city_idx < 0) {
        if (result->n_cities >= MAX_CITY_PER_THREAD) {
          break;
        }
        memcpy(result->results[result->n_cities].city, line_start, (size_t)city_len);
        result->results[result->n_cities].city[city_len] = '\0';
        result->results[result->n_cities].sum = temperature;
        result->results[result->n_cities].max = temperature;
        result->results[result->n_cities].min = temperature;
        result->results[result->n_cities].count = 1;
        htable[hash_idx] = result->n_cities;
        result->n_cities++;
      } else {
        result->results[city_idx].sum += temperature;
        result->results[city_idx].count += 1;
        if (result->results[city_idx].min > temperature) {
          result->results[city_idx].min = temperature;
        }
        if (result->results[city_idx].max < temperature) {
          result->results[city_idx].max = temperature;
        }
      }
    }
  }
  free(buffer);
  return (void *)result;
}


int main()
{
  FILE *fp = fopen("../measurements.txt", "r");
  if (fp == NULL) {
    perror("failed to open file");
    return 1;
  }

  pthread_t workers[N_THREADS];
  for (int i = 0; i < N_THREADS; i++) {
    pthread_create(&workers[i], NULL, process_chunk, fp);
  }

  ThreadResults *results[N_THREADS];
  for (int i = 0; i < N_THREADS; i++) {
    pthread_join(workers[i], (void *)&results[i]);
  }

  fclose(fp);

  ThreadResults stats;
  stats.n_cities = 0;
  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));

  for (int i = 0; i < N_THREADS; i++) {
    for (int j = 0; j < results[i]->n_cities; j++) {
      ThreadCityStats *b = &results[i]->results[j];

      unsigned int hash_idx = hash_fn((char *)b->city, (int)strlen(b->city)) % HT_CAPACITY;
      int city_idx = htable[hash_idx];
      while (city_idx != -1 && strcmp(stats.results[city_idx].city, b->city) != 0) {
        hash_idx = (hash_idx + 1) % HT_CAPACITY;
        city_idx = htable[hash_idx];
      }

      if (city_idx < 0) {
        strcpy(stats.results[stats.n_cities].city, b->city);
        stats.results[stats.n_cities].sum = b->sum;
        stats.results[stats.n_cities].max = b->max;
        stats.results[stats.n_cities].min = b->min;
        stats.results[stats.n_cities].count = 1;
        htable[hash_idx] = stats.n_cities;
        stats.n_cities++;
      } else {
        ThreadCityStats *a = &stats.results[city_idx];
        a->sum += b->sum;
        a->count += b->count;
        if (a->min > b->min) {
          a->min = b->min;
        } else if (a->max < b->max) {
          a->max = b->max;
        }
      }
    }

    free(results[i]);
  }


  qsort(stats.results, (size_t) stats.n_cities, sizeof(ThreadCityStats), cmp);

  printf("{");
  for (int i = 0; i < stats.n_cities; i++) {
      printf("%s=%.1f/%.1f/%.1f", 
             stats.results[i].city, 
             stats.results[i].min,
             stats.results[i].sum / stats.results[i].count, 
             stats.results[i].max);

      if (i < stats.n_cities - 1) {
          printf(", ");
      }
  }
  printf("}\n");

  
  return 0;
}
```

### 8.1 What is Parallel Processing?

So far, all our implementations have been **single-threaded**—they use only one CPU core at a time. Even though modern CPUs have multiple cores (my M3 has 8 cores), our previous code only utilized one of them, leaving the other 7 cores idle. This is like having 8 workers available but only asking one to do all the work.

**Parallel processing** (also called **multi-threading**) means dividing the work among multiple threads that can run simultaneously on different CPU cores. Think of it like a team of workers: instead of one person processing the entire file sequentially, we have multiple workers each processing different chunks of the file at the same time. If we have 8 workers and they can all work simultaneously, we can potentially finish the job much faster.

However, parallel processing introduces new challenges:

- **Coordination**: Multiple threads need to coordinate when accessing shared resources (like the file)
- **Synchronization**: We need to ensure threads don't interfere with each other—for example, two threads shouldn't read from the file at the same time in a way that causes data corruption
- **Result merging**: Each thread produces its own results, which we need to combine at the end

In this implementation, we use **pthreads** (POSIX threads), a standard library for multi-threading in C. The key mechanism for synchronization is a **mutex** (mutual exclusion lock), which ensures only one thread can access a shared resource (like the file) at a time.

### 8.2 What's Different from the Previous Version?

This version introduces multi-threading while keeping all the optimizations from the previous version (buffered I/O, custom parsing, hash tables). The main changes are architectural: we now have multiple threads working in parallel, each processing chunks of the file independently.

#### 8.2.1 New Header Files and Constants (Lines 1-12)

```c
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <pthread.h>

#define HT_CAPACITY 4096
#define BUFF_SIZE 1024*64
#define N_THREADS 8
#define MAX_CITY_PER_THREAD 450
```

**What's new:**
- `<pthread.h>`: Provides functions for creating and managing threads (`pthread_create`, `pthread_join`, `pthread_mutex_lock`, etc.)
- `<stddef.h>`: Provides `size_t` type definition
- `HT_CAPACITY 4096`: Increased from 450 to 4096 to accommodate potential hash collisions across threads
- `N_THREADS 8`: Defines how many threads we'll create (matching the 8 CPU cores)
- `MAX_CITY_PER_THREAD 450`: Maximum cities each thread might encounter (safety limit)

**Why it matters:** These constants configure our parallel processing setup. We create 8 threads to match our 8-core CPU, allowing us to utilize all available cores simultaneously.

#### 8.2.2 New Data Structures (Lines 14-26)

```c
typedef struct {
  char city[100];
  float sum;
  float min;
  float max;
  int count;
} ThreadCityStats;

typedef struct {
  int n_cities;
  ThreadCityStats results[MAX_CITY_PER_THREAD];
} ThreadResults;
```

**What's new:** We now have separate structures for thread-local results:
- `ThreadCityStats`: Same as `CityStats` from before, but renamed to indicate it's used per-thread
- `ThreadResults`: Contains an array of `ThreadCityStats` and a count—each thread maintains its own results

**Why it matters:** Each thread processes chunks independently and accumulates statistics in its own `ThreadResults` structure. This avoids the need for locks when updating statistics (since each thread has its own data), which would be a major performance bottleneck. We only need synchronization when reading from the file.

#### 8.2.3 Thread-Safe File Reading (Lines 60-75)

```c
size_t fread_chunked(char *dest, FILE *fh) {
  static pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
  pthread_mutex_lock(&lock);

  size_t nread = fread(dest, 1, BUFF_SIZE, fh);
  if (nread <= 0) {
    pthread_mutex_unlock(&lock);
    return nread;
  }

  long rewind = 0;
  while (dest[nread - 1] != '\n') {
    rewind--;
    nread--;
  }
  fseek(fh, rewind, SEEK_CUR);
  pthread_mutex_unlock(&lock);
  return nread;
}
```

**What's new:** This function wraps `fread()` with thread synchronization:
- `static pthread_mutex_t lock`: A mutex (lock) that ensures only one thread can execute this function at a time
- `pthread_mutex_lock(&lock)`: Acquires the lock—if another thread is already reading, this thread waits
- `pthread_mutex_unlock(&lock)`: Releases the lock, allowing the next waiting thread to proceed

**How it works:**
1. When a thread calls `fread_chunked()`, it first tries to acquire the lock
2. If the lock is available, it proceeds to read a chunk from the file
3. It handles partial lines (rewinding if needed) just like the previous version
4. It releases the lock, allowing other threads to read their chunks

**Why it's crucial:** Without the mutex, multiple threads could read from the file simultaneously, causing:
- Data corruption (threads reading overlapping or incomplete data)
- Race conditions (unpredictable behavior)
- Incorrect results

The mutex ensures that file reading is **serialized** (happens one at a time), but the actual processing of each chunk happens in parallel. This is a common pattern: serialize I/O operations (which are inherently sequential), but parallelize CPU-intensive work (parsing and computation).

#### 8.2.4 The Thread Worker Function (Lines 77-147)

```c
void *process_chunk(void *arg){
  FILE *fp = (FILE *)arg;

  ThreadResults *result = malloc(sizeof(*result));
  if (!result) {
    perror("malloc");
    exit(EXIT_FAILURE);
  }
  result->n_cities = 0;

  char *buffer = malloc(BUFF_SIZE);

  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));

  while (1) {
    size_t nread = fread_chunked(buffer, fp); 
    if (nread <= 0) {
      break;
    }
    // ... process the chunk ...
  }
  free(buffer);
  return (void *)result;
}
```

**What's new:** This function runs in each thread:
- **Parameter**: Takes a `void *` pointer (required by pthreads), which we cast to `FILE *` to pass the file handle
- **Return value**: Returns a `void *` pointer to `ThreadResults` (required by pthreads)
- **Thread-local storage**: Each thread allocates its own `ThreadResults`, buffer, and hash table
- **Loop**: Continuously reads chunks using `fread_chunked()` until the file is exhausted

**How it works:**
1. Each thread allocates memory for its results and buffer
2. It creates its own hash table (no sharing needed—each thread tracks cities independently)
3. In a loop, it calls `fread_chunked()` to get the next chunk (the mutex ensures threads don't interfere)
4. It processes the chunk: parses lines, updates statistics in its own hash table
5. When no more data is available (`nread <= 0`), it breaks and returns its results

**Why it's efficient:** Each thread maintains its own hash table and statistics, so there's no contention (no need for locks) when updating data. The only synchronization point is when reading from the file, which is necessary but happens relatively infrequently compared to the processing work.

**The processing logic** (lines 95-146) is similar to the previous version:
- Parses each line to extract city name and temperature
- Uses hash table lookup to find or create city entries
- Updates min, max, sum, and count for each city
- The key difference: all this happens independently in each thread

#### 8.2.5 Creating and Managing Threads (Lines 150-163)

```c
pthread_t workers[N_THREADS];
for (int i = 0; i < N_THREADS; i++) {
  pthread_create(&workers[i], NULL, process_chunk, fp);
}

ThreadResults *results[N_THREADS];
for (int i = 0; i < N_THREADS; i++) {
  pthread_join(workers[i], (void *)&results[i]);
}
```

**What's new:** The main function now creates and coordinates threads:
- `pthread_t workers[N_THREADS]`: Array to store thread identifiers
- `pthread_create()`: Creates a new thread that runs `process_chunk()` function
  - First argument: pointer to store the thread ID
  - Second argument: thread attributes (NULL for defaults)
  - Third argument: function to run (`process_chunk`)
  - Fourth argument: argument to pass to the function (the file pointer `fp`)
- `pthread_join()`: Waits for a thread to finish and retrieves its return value
  - Blocks until the thread completes
  - Stores the returned `ThreadResults` pointer in `results[i]`

**How it works:**
1. **Creation phase**: We create 8 threads, each running `process_chunk()` with the same file pointer
2. **Execution phase**: All 8 threads run simultaneously, each reading chunks and processing them independently
3. **Join phase**: We wait for all threads to finish and collect their results

**Why `pthread_join()` is necessary:** Without joining, the main thread might exit before worker threads finish, or we might try to access results before they're ready. `pthread_join()` ensures we wait for completion and safely retrieve results.

#### 8.2.6 Merging Results from All Threads (Lines 167-200)

```c
ThreadResults stats;
stats.n_cities = 0;
int htable[HT_CAPACITY];
memset(htable, -1, HT_CAPACITY * sizeof(int));

for (int i = 0; i < N_THREADS; i++) {
  for (int j = 0; j < results[i]->n_cities; j++) {
    ThreadCityStats *b = &results[i]->results[j];

    unsigned int hash_idx = hash_fn((char *)b->city, (int)strlen(b->city)) % HT_CAPACITY;
    int city_idx = htable[hash_idx];
    while (city_idx != -1 && strcmp(stats.results[city_idx].city, b->city) != 0) {
      hash_idx = (hash_idx + 1) % HT_CAPACITY;
      city_idx = htable[hash_idx];
    }

    if (city_idx < 0) {
      strcpy(stats.results[stats.n_cities].city, b->city);
      stats.results[stats.n_cities].sum = b->sum;
      stats.results[stats.n_cities].max = b->max;
      stats.results[stats.n_cities].min = b->min;
      stats.results[stats.n_cities].count = 1;
      htable[hash_idx] = stats.n_cities;
      stats.n_cities++;
    } else {
      ThreadCityStats *a = &stats.results[city_idx];
      a->sum += b->sum;
      a->count += b->count;
      if (a->min > b->min) {
        a->min = b->min;
      } else if (a->max < b->max) {
        a->max = b->max;
      }
    }
  }

  free(results[i]);
}
```

**What's new:** After all threads finish, we need to combine their results:
- We create a new `ThreadResults` structure (`stats`) to hold the final merged results
- We iterate through each thread's results
- For each city in each thread's results, we either:
  - **Add it as new**: If we haven't seen this city yet, copy it to the final results
  - **Merge statistics**: If we've seen this city from another thread, combine the statistics:
    - Add the sums and counts
    - Take the minimum of the two minimums
    - Take the maximum of the two maximums

**Why merging is necessary:** Different threads process different chunks of the file, so the same city might appear in multiple threads' results. For example, "Tokyo" might appear in chunks processed by thread 1, thread 3, and thread 7. We need to combine all these occurrences to get the correct overall statistics.

**How merging works:**
- We use a hash table (just like before) to quickly find if we've already seen a city
- When merging, we combine sums and counts (for calculating the average)
- We update min/max by comparing values from different threads
- The final `stats` structure contains the complete, merged statistics for all cities

**Example of merging:**
Suppose "Tokyo" appears in three threads' results:
- Thread 1: sum=100.0, count=3, min=30.0, max=35.0
- Thread 3: sum=50.0, count=2, min=25.0, max=30.0  
- Thread 7: sum=75.0, count=3, min=20.0, max=28.0

When merging:
- First, we add Thread 1's data: sum=100.0, count=3, min=30.0, max=35.0
- Then we merge Thread 3: sum=100.0+50.0=150.0, count=3+2=5, min=min(30.0,25.0)=25.0, max=max(35.0,30.0)=35.0
- Finally, we merge Thread 7: sum=150.0+75.0=225.0, count=5+3=8, min=min(25.0,20.0)=20.0, max=max(35.0,28.0)=35.0
- Result: average = 225.0/8 = 28.125, min=20.0, max=35.0

**Performance consideration:** Merging happens sequentially in the main thread after all parallel work is done. Since we're only merging 8 sets of results (one per thread), and each set contains at most 450 cities, this is very fast compared to processing 1 billion rows.

### 8.3 Performance Improvement

Running this parallel processing version on the full dataset shows dramatic improvement:

```bash
./main  20.51s user 2.96s system 428% cpu 5.474 total
```

Compared to the buffered I/O version (section 7):
- **User time**: Increased from 19.32 seconds to 20.51 seconds (~6% increase)
- **System time**: Increased from 1.26 seconds to 2.96 seconds (~135% increase)
- **CPU usage**: Increased from 99% to 428% (indicating multiple cores working!)
- **Total elapsed time**: Reduced from ~20.7 seconds to ~5.5 seconds (~3.8x faster!)

**Understanding the metrics:**

The **user time increase** (19.32s → 20.51s) might seem counterintuitive, but it makes sense: user time is the sum of CPU time used by all threads. With 8 threads running simultaneously, we're doing more total work (each thread processes chunks independently), but we're doing it in parallel. Think of it like this: if 8 workers each work for 2.5 seconds, the total "worker time" is 20 seconds, but the job finishes in 2.5 seconds because they work simultaneously.

The **system time increase** (1.26s → 2.96s) reflects additional overhead from:
- Thread creation and management
- Mutex operations (acquiring and releasing locks)
- More frequent file operations as threads compete for file access
- Context switching between threads

The **CPU usage of 428%** is the key indicator: it means we're using approximately 4.28 CPU cores on average (out of 8 available). This is less than the ideal 800% (8 cores × 100%) because:
- Some time is spent waiting for the mutex (only one thread can read at a time)
- Thread synchronization overhead
- The final merging step happens sequentially

The **total elapsed time reduction** (20.7s → 5.5s) is what matters most—the wall-clock time it takes to complete the task. Even though we're doing more total work and have more overhead, parallel execution makes the program finish **3.8x faster**.

**Why not 8x faster?** Perfect 8x speedup would require:
- Zero synchronization overhead (impossible—we need the mutex for file access)
- Perfect load balancing (all threads finish at the same time)
- No sequential bottlenecks (but merging must happen sequentially)
- No context switching overhead

In practice, achieving 3.8x speedup with 8 threads is excellent performance. The mutex serializes file I/O (which is necessary), but the CPU-intensive parsing and hash table operations happen in parallel, giving us the speedup.

This version processes 1 billion rows in just **5.5 seconds**—down from nearly 8 minutes in our first naive implementation. That's a **85x speedup** from where we started, achieved through understanding parallel processing, thread synchronization, and how to effectively utilize multiple CPU cores.

## 9. MMAP and Code Optimization

```c
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <pthread.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <unistd.h>

#define HT_CAPACITY 4096
#define BUFF_SIZE 1024*64
#define N_THREADS 8
#define MAX_CITY_PER_THREAD 450

#define min(a, b) (a ^ ((b ^ a) & -(b < a)))
#define max(a, b) (a ^ ((a ^ b) & -(a < b)))


typedef struct {
  char city[100];
  long sum;
  int min;
  int max;
  int count;
} ThreadCityStats;

typedef struct {
  int n_cities;
  ThreadCityStats results[MAX_CITY_PER_THREAD];
} ThreadResults;

struct Chunk {
  size_t start;
  size_t end;
  char *data;
};

static unsigned int hash_fn(const unsigned char *data, int n) {
  unsigned int hash = 0;
  for (int i = 0; i < n; i++) {
    hash = (hash * 31) + data[i];
  }
  return hash;
}

static inline const char *parse_number(int *dest, const char *s) {
  int mod;
  if (*s == '-') {
    mod = -1;
    s++;
  } else {
    mod = 1;
  }

  if (s[1] == '.') {
    *dest = ((s[0] * 10) + s[2] - ('0' * 11)) * mod;
    return s + 4;
  }

  *dest = (s[0] * 100 + s[1] * 10 + s[3] - '0' * 111) * mod;
  return s + 5;
}


static int cmp(const void *ptr_a, const void *ptr_b) {
  return strcmp(((ThreadCityStats *)ptr_a)->city, ((ThreadCityStats*)ptr_b)->city);
}

void result_to_str(char *dest, const ThreadResults *result) {
  char buf[128];
  *dest++ = '{';
  for (int i = 0; i < result->n_cities; i++) {
    size_t n = (size_t)sprintf(
        buf, "%s=%.1f/%.1f/%.1f", result->results[i].city,
        (float)result->results[i].min / 10.0,
        ((float)result->results[i].sum / (float)result->results[i].count) / 10.0,
        (float)result->results[i].max / 10.0);

    memcpy(dest, buf, n);
    if (i < result->n_cities - 1) {
      memcpy(dest + n, ", ", 2);
      n += 2;
    }

    dest += n;
  }
  *dest++ = '}';
  *dest = 0x0;
}


void *process_chunk(void *arg){
  struct Chunk *ch = (struct Chunk *)arg;

  if (ch->start > 0) {
    while (ch->data[ch->start - 1] != '\n') {
      ch->start++;
    }
  }

  while (ch->data[ch->end] != 0x0 && ch->data[ch->end - 1] != '\n') {
    ch->end++;
  }

  ThreadResults *result = malloc(sizeof(*result));
  if (!result) {
    perror("malloc");
    exit(EXIT_FAILURE);
  }
  result->n_cities = 0;

  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));

  const char *s = &ch->data[ch->start];
  const char *end = &ch->data[ch->end];
  const char *line_start;
  unsigned int h;
  int temperature;
  int len;
  int city_idx;

  while (s != end) {
    line_start = s;

    len = 1;
    h = (unsigned char)s[0];
    while (s[len] != ';') {
      h = (h * 31) + (unsigned char)s[len++];
    }

    s = parse_number(&temperature, s + len + 1);

    h = h & (HT_CAPACITY - 1);
    while (htable[h] >= 0 && memcmp(result->results[htable[h]].city,
                                     line_start, (size_t)len) != 0) {
      h = (h + 1) & (HT_CAPACITY - 1);
    }
    city_idx = htable[h];

    if (city_idx < 0) {
      if (result->n_cities >= MAX_CITY_PER_THREAD) {
        break;
      }
      memcpy(result->results[result->n_cities].city, line_start, (size_t)len);
      result->results[result->n_cities].city[len] = '\0';
      result->results[result->n_cities].sum = temperature;
      result->results[result->n_cities].max = temperature;
      result->results[result->n_cities].min = temperature;
      result->results[result->n_cities].count = 1;
      htable[h] = result->n_cities;
      result->n_cities++;
    } else {
      result->results[city_idx].count += 1;
      result->results[city_idx].sum += temperature;
      result->results[city_idx].min = min(result->results[city_idx].min, temperature);
      result->results[city_idx].max = max(result->results[city_idx].max, temperature);
    }
  }
  return (void *)result;
}


int main()
{
  const char *file = "../measurements.txt";
  int fd = open(file, O_RDONLY);
  if (fd < 0) {
    perror("error opening file");
    exit(EXIT_FAILURE);
  }

  struct stat sb;
  if (fstat(fd, &sb) == -1) {
    perror("error getting file size");
    exit(EXIT_FAILURE);
  }

  size_t sz = (size_t)sb.st_size;
  char *data = mmap(NULL, sz, PROT_READ, MAP_SHARED, fd, 0);
  if (data == MAP_FAILED) {
    perror("error mmapping file");
    exit(EXIT_FAILURE);
  }

  pthread_t workers[N_THREADS];
  struct Chunk chunks[N_THREADS];
  size_t chunk_size = sz / (size_t)N_THREADS;
  for (int i = 0; i < N_THREADS; i++) {
    chunks[i].data = data;
    chunks[i].start = chunk_size * (size_t)i;
    chunks[i].end = (i == N_THREADS - 1) ? sz : chunk_size * ((size_t)i + 1);
    pthread_create(&workers[i], NULL, process_chunk, &chunks[i]);
  }

  ThreadResults *results[N_THREADS];
  for (int i = 0; i < N_THREADS; i++) {
    pthread_join(workers[i], (void *)&results[i]);
  }

  munmap((void *)data, sz);
  close(fd);

  ThreadResults stats;
  stats.n_cities = 0;
  int htable[HT_CAPACITY];
  memset(htable, -1, HT_CAPACITY * sizeof(int));

  for (int i = 0; i < N_THREADS; i++) {
    for (int j = 0; j < results[i]->n_cities; j++) {
      ThreadCityStats *b = &results[i]->results[j];

      unsigned int h = hash_fn((const unsigned char *)b->city, (int)strlen(b->city)) & (HT_CAPACITY - 1);
      int c = htable[h];
      while (c != -1 && strcmp(stats.results[c].city, b->city) != 0) {
        h = (h + 1) & (HT_CAPACITY - 1);
        c = htable[h];
      }

      if (c < 0) {
        strcpy(stats.results[stats.n_cities].city, b->city);
        stats.results[stats.n_cities].sum = b->sum;
        stats.results[stats.n_cities].max = b->max;
        stats.results[stats.n_cities].min = b->min;
        stats.results[stats.n_cities].count = 1;
        htable[h] = stats.n_cities;
        stats.n_cities++;
      } else {
        ThreadCityStats *a = &stats.results[c];
        a->sum += b->sum;
        a->count += b->count;
        a->min = min(a->min, b->min);
        a->max = max(a->max, b->max);
      }
    }

    free(results[i]);
  }


  qsort(stats.results, (size_t) stats.n_cities, sizeof(ThreadCityStats), cmp);

  char buf[(1 << 10) * 16];
  result_to_str(buf, &stats);
  puts(buf);

  
  return 0;
}
```

### 9.1 What is Memory-Mapped I/O?

In all previous versions, we used `fread()` to read chunks of data from the file into a buffer. While this is efficient compared to line-by-line reading, it still requires explicit system calls to copy data from the file into our program's memory. Each `fread()` call involves:
- A system call to the kernel
- Copying data from kernel space to user space
- Managing file position pointers
- Handling buffer management

**Memory-mapped I/O** (using `mmap()`) takes a fundamentally different approach: instead of copying data from the file into our memory, we tell the operating system to map the file directly into our process's virtual address space. The file appears as if it's already in memory, but the OS handles loading pages from disk on-demand.

Think of it like this: with `fread()`, you're asking the OS to bring you a book page by page, and you read each page. With `mmap()`, you're asking the OS to make the entire book appear on your desk—you can read any page instantly, and the OS handles fetching pages from the library (disk) when you need them.

**Key benefits of memory-mapped I/O:**
- **No explicit copying**: The OS handles data transfer automatically through the virtual memory system
- **Efficient paging**: The OS can optimize which pages to keep in RAM based on access patterns
- **Simpler code**: No need to manage buffers or handle partial reads—the file is just a pointer
- **Better cache utilization**: The OS can prefetch pages and manage cache more intelligently
- **Zero-copy potential**: Data doesn't need to be copied from kernel buffers to user buffers

**Trade-offs:**
- The entire file is mapped into virtual memory (though not necessarily loaded into physical RAM)
- We need to handle the file size explicitly (get file size with `fstat()` before mapping)
- On 32-bit systems, large files might not fit in the address space (not an issue on 64-bit systems)

### 9.2 What's Different from the Previous Version?

This version introduces several major optimizations beyond memory-mapped I/O. Let's examine each change:

#### 9.2.1 Memory-Mapped Files with `mmap()`

```c
int fd = open(file, O_RDONLY);
if (fd < 0) {
  perror("error opening file");
  exit(EXIT_FAILURE);
}

struct stat sb;
if (fstat(fd, &sb) == -1) {
  perror("error getting file size");
  exit(EXIT_FAILURE);
}

size_t sz = (size_t)sb.st_size;
char *data = mmap(NULL, sz, PROT_READ, MAP_SHARED, fd, 0);
if (data == MAP_FAILED) {
  perror("error mmapping file");
  exit(EXIT_FAILURE);
}
```

**What's new:** Instead of opening a file with `fopen()` and reading with `fread()`, we now:
- Use `open()` to get a file descriptor (lower-level than `FILE *`)
- Call `fstat()` to get the file size (needed for `mmap()`)
- Call `mmap()` to map the entire file into memory:
  - `NULL`: Let the OS choose where to map it
  - `sz`: Map the entire file size
  - `PROT_READ`: Map as read-only (we're not modifying the file)
  - `MAP_SHARED`: Share the mapping with other processes (standard for file mappings)
  - `fd`: The file descriptor
  - `0`: Offset of 0 (start from beginning)

**How it works:** After `mmap()` succeeds, `data` points to the beginning of the file in memory. We can access `data[0]` through `data[sz-1]` as if the file were already loaded. The OS handles loading pages from disk when we access them.

**Why it's faster:**
- **Eliminates buffer management**: No need to allocate buffers or manage read positions
- **Reduces system calls**: No repeated `fread()` calls—the OS handles paging automatically
- **Better prefetching**: The OS can predict access patterns and prefetch pages
- **Simpler code**: The file is just a pointer—no need to track buffer positions or handle partial reads

**Cleanup:**
```c
munmap((void *)data, sz);
close(fd);
```

We must unmap the memory and close the file descriptor when done. `munmap()` releases the virtual memory mapping.

#### 9.2.2 Integer-Based Temperature Storage

```c
typedef struct {
  char city[100];
  long sum;
  int min;
  int max;
  int count;
} ThreadCityStats;
```

**What's different:** Previously, temperatures were stored as `float` values. Now they're stored as `int` values, with temperatures multiplied by 10 (so 35.6 becomes 356).

**The parsing function:**
```c
static inline const char *parse_number(int *dest, const char *s) {
  int mod;
  if (*s == '-') {
    mod = -1;
    s++;
  } else {
    mod = 1;
  }

  if (s[1] == '.') {
    *dest = ((s[0] * 10) + s[2] - ('0' * 11)) * mod;
    return s + 4;
  }

  *dest = (s[0] * 100 + s[1] * 10 + s[3] - '0' * 111) * mod;
  return s + 5;
}
```

**How it works:**

The key insight is that we're working with ASCII character codes, not numeric values. To convert a character digit to its numeric value, we subtract `'0'` (which is ASCII 48). The formulas use a clever trick to combine multiple character-to-number conversions into a single expression.

**For single-digit temperatures like "5.6":**
- `s[0]` is '5' (ASCII 53), `s[2]` is '6' (ASCII 54)
- The formula `(s[0] * 10) + s[2] - ('0' * 11)` works as follows:
  - `s[0] * 10` = `53 * 10` = `530` (we'll subtract the ASCII offset later)
  - `s[2]` = `54` (we'll subtract the ASCII offset later)
  - `'0' * 11` = `48 * 11` = `528`
  - Result: `530 + 54 - 528` = `56` (which represents 5.6 when divided by 10)
- More intuitively: `(s[0] - '0') * 10 + (s[2] - '0')` = `(53 - 48) * 10 + (54 - 48)` = `5 * 10 + 6` = `56` ✓
- The code's formula is equivalent but avoids multiple subtractions

**For two-digit temperatures like "35.6":**
- `s[0]` is '3' (ASCII 51), `s[1]` is '5' (ASCII 53), `s[3]` is '6' (ASCII 54)
- The formula `(s[0] * 100) + (s[1] * 10) + s[3] - ('0' * 111)` works as follows:
  - `s[0] * 100` = `51 * 100` = `5100`
  - `s[1] * 10` = `53 * 10` = `530`
  - `s[3]` = `54`
  - `'0' * 111` = `48 * 111` = `5328` (this accounts for subtracting '0' from all three digits: 48*100 + 48*10 + 48*1 = 5328)
  - Result: `5100 + 530 + 54 - 5328` = `356` (which represents 35.6 when divided by 10) ✓
- More intuitively: `(s[0] - '0') * 100 + (s[1] - '0') * 10 + (s[3] - '0')` = `(51 - 48) * 100 + (53 - 48) * 10 + (54 - 48)` = `3 * 100 + 5 * 10 + 6` = `356` ✓

**Why it's faster:**
- **No floating-point operations**: Integer arithmetic is faster than floating-point on most CPUs
- **No precision issues**: Integer math is exact (no rounding errors during accumulation)
- **Simpler comparisons**: Integer comparisons are faster than floating-point comparisons
- **Better cache behavior**: Integers are smaller (4 bytes vs 4 bytes for float, but no floating-point unit overhead)

**Conversion back to float:**
```c
(float)result->results[i].min / 10.0,
((float)result->results[i].sum / (float)result->results[i].count) / 10.0,
(float)result->results[i].max / 10.0
```

We only convert to float when formatting output, minimizing floating-point operations.

#### 9.2.3 Bitwise Min/Max Operations

```c
#define min(a, b) (a ^ ((b ^ a) & -(b < a)))
#define max(a, b) (a ^ ((a ^ b) & -(a < b)))
```

**What's new:** Instead of using conditional statements like `if (a < b) min = a; else min = b;`, we use bitwise operations.

**How it works:**

Let's break down the `min(a, b)` macro step by step:
- `(b < a)` evaluates to 1 if `b < a` is true, 0 if false
- `-(b < a)` converts this to -1 (all bits set, i.e., 0xFFFFFFFF) if true, or 0 if false
  - In two's complement, `-1` has all bits set to 1
- `(b ^ a)` computes the XOR of `b` and `a` (bits that differ between the two values)
- `(b ^ a) & -(b < a)` gives us:
  - `(b ^ a)` if `b < a` (because `&` with all 1s preserves the value)
  - `0` if `b >= a` (because `&` with 0 clears everything)
- `a ^ ((b ^ a) & -(b < a))` gives us:
  - `a ^ (b ^ a) = b` if `b < a` (because XOR is associative and `a ^ a = 0`, so `a ^ (b ^ a) = (a ^ a) ^ b = 0 ^ b = b`)
  - `a ^ 0 = a` if `b >= a`

**Example:** If `a = 10` and `b = 5`:
- `(b < a)` = `(5 < 10)` = `1` (true)
- `-(b < a)` = `-1` = `0xFFFFFFFF`
- `(b ^ a)` = `5 ^ 10` = `15` (binary: `0101 ^ 1010` = `1111`)
- `(b ^ a) & -(b < a)` = `15 & 0xFFFFFFFF` = `15`
- `a ^ 15` = `10 ^ 15` = `5` ✓ (returns the minimum, which is `b`)

For `max(a, b)`, the logic is similar but uses `(a < b)` instead.

**Why it's faster:**
- **No branch misprediction**: Conditional branches can cause CPU pipeline stalls when the branch predictor guesses wrong. Bitwise operations are always predictable.
- **Single expression**: The CPU can execute this as a single instruction sequence without branching
- **Better for pipelining**: Modern CPUs can execute bitwise operations very efficiently in parallel

**Trade-off:** This is less readable than `if/else`, but the performance gain is worth it when called billions of times.

#### 9.2.4 Fixed Chunk Division (Eliminating Mutex)

```c
pthread_t workers[N_THREADS];
struct Chunk chunks[N_THREADS];
size_t chunk_size = sz / (size_t)N_THREADS;
for (int i = 0; i < N_THREADS; i++) {
  chunks[i].data = data;
  chunks[i].start = chunk_size * (size_t)i;
  chunks[i].end = (i == N_THREADS - 1) ? sz : chunk_size * ((size_t)i + 1);
  pthread_create(&workers[i], NULL, process_chunk, &chunks[i]);
}
```

**What's different:** In the previous version, threads competed for chunks using a mutex-protected `fread_chunked()` function. Now, we divide the file into fixed chunks upfront, and each thread processes its assigned chunk independently.

**How it works:**
- We calculate `chunk_size = sz / N_THREADS` to divide the file evenly
- Each thread gets a `Chunk` structure with:
  - `data`: Pointer to the memory-mapped file (same for all threads)
  - `start`: Starting byte offset for this thread's chunk
  - `end`: Ending byte offset for this thread's chunk
- Threads process their chunks completely independently—no mutex needed!

**Chunk boundary handling:**
```c
if (ch->start > 0) {
  while (ch->data[ch->start - 1] != '\n') {
    ch->start++;
  }
}

while (ch->data[ch->end] != 0x0 && ch->data[ch->end - 1] != '\n') {
  ch->end++;
}
```

Each thread adjusts its boundaries to start and end on newline characters, ensuring we don't split lines across threads. Here's how it works:

**Starting boundary adjustment:**
- If `ch->start > 0` (not the first chunk), we need to ensure we start at the beginning of a line
- We check the character before our start position (`ch->data[ch->start - 1]`)
- If it's not a newline, we're in the middle of a line, so we increment `start` until we find a newline
- This moves the start position forward to the beginning of the next complete line

**Ending boundary adjustment:**
- We check if the character at our end position (`ch->data[ch->end]`) is not null and the character before it (`ch->data[ch->end - 1]`) is not a newline
- If so, we're in the middle of a line, so we increment `end` until we find a newline
- This extends the end position to include the complete line that was cut off

**Example:** 
- Thread 1's chunk might be calculated to end at byte 1,000,000, but that byte is in the middle of "Tokyo;35.6\n"
- The ending boundary adjustment finds the newline after "35.6" and extends the chunk to include the complete "Tokyo" line
- Thread 2's chunk then starts after that newline, ensuring no line is processed twice or split incorrectly

**Why it's faster:**
- **No mutex contention**: Threads never wait for each other—they work completely independently
- **Better cache locality**: Each thread works on a contiguous region of memory
- **Simpler code**: No need for mutex locks or thread-safe file reading
- **Better load balancing**: All threads start simultaneously and process similar-sized chunks

**Trade-off:** If chunks have different numbers of lines (due to varying line lengths), some threads might finish earlier than others. However, this is still faster than mutex contention.

#### 9.2.5 Optimized Parsing Function

The `parse_number()` function is now `inline` and returns a pointer to continue parsing:

```c
static inline const char *parse_number(int *dest, const char *s) {
  // ... parsing logic ...
  return s + 4;  // or s + 5
}
```

**What's different:** 
- Function is marked `inline`, suggesting the compiler should inline it (eliminate function call overhead)
- Returns a pointer to the next character after the temperature, eliminating the need to search for the newline

**How it's used:**
```c
s = parse_number(&temperature, s + len + 1);
```

After parsing, `s` points directly to the newline character, ready for the next iteration.

**Why it's faster:**
- **Inlining**: The compiler can inline the function, eliminating call overhead
- **No search needed**: We know exactly where the temperature ends, so we don't need `strchr()` to find the newline
- **Fewer operations**: Direct pointer arithmetic instead of string searching

#### 9.2.6 Optimized Output Formatting

```c
void result_to_str(char *dest, const ThreadResults *result) {
  char buf[128];
  *dest++ = '{';
  for (int i = 0; i < result->n_cities; i++) {
    size_t n = (size_t)sprintf(
        buf, "%s=%.1f/%.1f/%.1f", result->results[i].city,
        (float)result->results[i].min / 10.0,
        ((float)result->results[i].sum / (float)result->results[i].count) / 10.0,
        (float)result->results[i].max / 10.0);

    memcpy(dest, buf, n);
    if (i < result->n_cities - 1) {
      memcpy(dest + n, ", ", 2);
      n += 2;
    }

    dest += n;
  }
  *dest++ = '}';
  *dest = 0x0;
}
```

**What's different:** Instead of calling `printf()` multiple times (which involves system calls), we build the entire output string in memory using `sprintf()` and `memcpy()`, then output it once with `puts()`.

**How it works:**
- We allocate a large buffer `buf[(1 << 10) * 16]` (16KB) to hold the output
- For each city, we format it into a temporary buffer with `sprintf()`
- We copy the formatted string into the destination buffer with `memcpy()`
- We add commas between cities
- Finally, we output the entire string with a single `puts()` call

**Why it's faster:**
- **Fewer system calls**: One `puts()` call instead of many `printf()` calls
- **Better buffering**: We control the buffering explicitly
- **Reduced overhead**: `memcpy()` is faster than repeated `printf()` formatting

### 9.3 Performance Improvement

Running this memory-mapped and optimized version on the full dataset shows significant improvement:

```bash
./main 16.84s user 0.94s system 757% cpu 2.348 total
```

Compared to the parallel processing version (section 8):
- **User time**: Reduced from 20.51 seconds to 16.84 seconds (~18% faster)
- **System time**: Reduced from 2.96 seconds to 0.94 seconds (~68% reduction!)
- **CPU usage**: Increased from 428% to 757% (better core utilization!)
- **Total elapsed time**: Reduced from ~5.5 seconds to ~2.35 seconds (~2.3x faster!)

**Understanding the improvements:**

The **system time reduction** (2.96s → 0.94s) is dramatic and directly reflects the benefits of memory-mapped I/O:
- No explicit `fread()` system calls—the OS handles paging automatically
- No mutex operations for file access—threads work independently
- Reduced kernel overhead from fewer system calls

The **CPU usage increase** (428% → 757%) shows we're utilizing more CPU cores effectively:
- With no mutex contention, threads spend more time computing and less time waiting
- Better cache behavior from contiguous memory access patterns
- More efficient integer operations vs floating-point

The **user time reduction** (20.51s → 16.84s) comes from:
- Integer arithmetic instead of floating-point (faster operations)
- Bitwise min/max operations (no branch mispredictions)
- Inlined parsing function (no call overhead)
- Optimized output formatting (fewer function calls)

The **total elapsed time reduction** (5.5s → 2.35s) is the most important metric—the wall-clock time to complete the task. This **2.3x speedup** comes from eliminating mutex contention and optimizing the hot paths in our code.

**Why memory-mapped I/O helps:**
- The OS can prefetch pages based on access patterns
- No explicit buffer management overhead
- Better integration with the virtual memory system
- Reduced context switching between user and kernel space

**Why integer storage helps:**
- Integer arithmetic is faster than floating-point on most CPUs
- No floating-point unit pipeline stalls
- Simpler operations for the CPU to execute

**Why bitwise min/max helps:**
- Eliminates branch mispredictions that can stall the CPU pipeline
- More predictable execution pattern for the CPU's branch predictor
- Better instruction-level parallelism

This version processes 1 billion rows in just **2.35 seconds**—down from nearly 8 minutes in our first naive implementation. That's a **200x speedup** from where we started, achieved through understanding memory-mapped I/O, eliminating synchronization overhead, and optimizing every hot path in the code.

### 9.4 Performance Comparison Summary

Here's a summary of all optimization versions and their performance improvements:

| Version | Key Optimizations | User Time | System Time | Total Time | CPU Usage | Speedup vs Naive |
|---------|------------------|-----------|-------------|------------|-----------|------------------|
| **1. Naive** | Linear search, line-by-line reading | 465.77s | 4.25s | ~470s (7:50) | 99% | 1x (baseline) |
| **2. Hash Table** | Hash table lookups | 64.78s | 2.35s | ~68s (1:08) | 98% | 6.9x |
| **3. Custom Parsing** | Specialized float parser | 54.86s | 2.37s | ~57s | 99% | 8.2x |
| **4. Buffered I/O** | 64KB buffer reads, memcmp/memcpy | 19.32s | 1.26s | ~21s | 99% | 22.4x |
| **5. Parallel Processing** | 8 threads, mutex-protected reads | 20.51s | 2.96s | ~5.5s | 428% | 85.5x |
| **6. MMAP + Optimizations** | Memory-mapped I/O, integer math, bitwise ops | 16.84s | 0.94s | ~2.35s | 757% | 200x |

**Key insights from the optimization journey:**

- **Hash tables** provided the biggest single optimization (6.9x), eliminating O(n) linear searches
- **Buffered I/O** dramatically reduced system call overhead (22.4x total speedup)
- **Parallel processing** leveraged multiple CPU cores effectively (3.8x improvement over buffered version)
- **Memory-mapped I/O** eliminated mutex contention and reduced system time by 68% (2.3x improvement over parallel version)
- **Integer arithmetic and bitwise operations** provided incremental but important gains in the hot path

The progression from 470 seconds to 2.35 seconds demonstrates how understanding low-level computer architecture and making informed optimization decisions can yield dramatic performance improvements.

## 10. Conclusion

This implementation may not be the most optimized code in the world, but it's sufficient for my learning goals and demonstrates a deep understanding of how computers work at a low level. If I learn new optimization techniques in future projects that could further improve this code, I'll certainly revisit it. However, achieving a 200x speedup from the naive implementation while learning so much along the way feels like a significant accomplishment.

As someone who was practically a beginner in C before this project, I've learned an enormous amount. Here's an exhaustive list of everything I've learned:

### C Language Fundamentals

- **Structures (`struct`)**: How to define and use custom data types to organize related data
- **Typedef**: Creating type aliases for cleaner code
- **Pointers**: Understanding memory addresses, pointer arithmetic, and how pointers enable efficient data manipulation
- **Arrays**: Static arrays, array indexing, and how arrays relate to pointers
- **Functions**: Function definitions, parameters, return values, and the difference between `void` and other return types
- **Memory management**: Stack vs heap allocation, `malloc()` and `free()`, understanding memory layout
- **Header files**: Including standard library headers (`stdio.h`, `stdlib.h`, `string.h`, `pthread.h`, etc.) and understanding what functions they provide
- **Static functions**: Using `static` to limit function scope to the current file
- **Inline functions**: Using `inline` keyword to suggest compiler inlining for performance
- **Type casting**: Converting between types, especially between `char*` and other pointer types
- **Const correctness**: Using `const` to indicate read-only data
- **Void pointers**: Understanding `void*` and how to cast them for generic programming (as required by pthreads)

### File I/O Operations

- **`fopen()` and `fclose()`**: Opening and closing files, understanding file modes (`"r"` for read)
- **`fgets()`**: Reading files line by line, understanding buffer sizes and null termination
- **`fread()`**: Reading large chunks of data efficiently, understanding return values and partial reads
- **`fseek()`**: Repositioning file pointers, using `SEEK_CUR` for relative positioning
- **Buffered I/O**: Understanding the performance benefits of reading large chunks vs. line-by-line
- **File descriptors**: Using `open()` instead of `fopen()` for lower-level file access
- **`fstat()`**: Getting file metadata, especially file size needed for memory mapping
- **Memory-mapped I/O (`mmap()`)**: Mapping files directly into virtual memory, understanding `PROT_READ`, `MAP_SHARED`, and the benefits over explicit reads
- **`munmap()`**: Properly cleaning up memory mappings
- **Partial line handling**: Dealing with lines that span buffer boundaries when reading in chunks

### Data Structures and Algorithms

- **Hash tables**: Understanding hash tables as a data structure for O(1) average-case lookups
- **Hash functions**: Writing hash functions, understanding the importance of good distribution (using prime number 31)
- **Collision handling**: Implementing linear probing to handle hash collisions
- **Hash table capacity**: Choosing appropriate sizes (450, 4096) and understanding load factors
- **Modulo operation**: Using `%` for hash table indexing and understanding power-of-two optimizations (`& (CAPACITY - 1)`)
- **Linear search**: Understanding O(n) complexity and why it's slow for large datasets
- **Sorting**: Using `qsort()` with custom comparison functions
- **Comparison functions**: Writing comparator functions for sorting, understanding the return value semantics

### String Operations

- **`strcmp()`**: Comparing null-terminated strings
- **`memcmp()`**: Comparing fixed-length byte sequences without null terminators, understanding when it's more efficient
- **`strcpy()`**: Copying null-terminated strings
- **`memcpy()`**: Copying fixed-length byte sequences, understanding when explicit length is better
- **`strchr()`**: Finding characters in strings, understanding return values (pointer or NULL)
- **`strlen()`**: Getting string length, understanding it scans until null terminator
- **`memset()`**: Initializing memory regions with a specific value
- **`sprintf()`**: Formatting strings into buffers
- **Null termination**: Understanding how C strings work with `\0` terminators
- **String parsing**: Manually parsing strings by character position and arithmetic
- **Custom parsing**: Writing specialized parsers that outperform general-purpose library functions

### Parallel Programming

- **Pthreads**: Using POSIX threads library for multi-threading
- **`pthread_create()`**: Creating threads, understanding function signatures (`void*` parameters and returns)
- **`pthread_join()`**: Waiting for threads to complete and retrieving their results
- **Thread-local storage**: Each thread maintaining its own data structures to avoid contention
- **Mutexes**: Understanding mutual exclusion locks (`pthread_mutex_t`)
- **`pthread_mutex_lock()` and `pthread_mutex_unlock()`**: Synchronizing access to shared resources
- **Thread synchronization**: Understanding when synchronization is needed (file I/O) vs. when it's not (independent processing)
- **Race conditions**: Understanding how multiple threads can cause unpredictable behavior without proper synchronization
- **Result merging**: Combining results from multiple threads, understanding how to aggregate statistics correctly
- **Load balancing**: Dividing work among threads, understanding fixed chunk division vs. dynamic work stealing
- **Thread safety**: Understanding what operations are thread-safe and which require synchronization

### Performance Optimization

- **Profiling**: Using `time` command to measure user time, system time, and total elapsed time
- **CPU usage metrics**: Understanding what CPU percentages mean (99%, 428%, 757%) and how they relate to core utilization
- **System calls**: Understanding the overhead of system calls and how to minimize them
- **Context switching**: Understanding the cost of switching between user space and kernel space
- **Cache behavior**: Understanding how memory access patterns affect CPU cache performance
- **Branch prediction**: Understanding how CPU branch predictors work and how branch mispredictions cause pipeline stalls
- **Instruction-level parallelism**: Understanding how CPUs execute multiple instructions simultaneously
- **Hot paths**: Identifying the most frequently executed code sections for optimization
- **Benchmarking**: Comparing performance between different implementations
- **Performance metrics**: Understanding the difference between wall-clock time, CPU time, and system time

### System Programming

- **System calls**: Understanding the difference between library functions and system calls
- **File descriptors**: Lower-level file access vs. `FILE*` streams
- **Virtual memory**: Understanding how `mmap()` uses the virtual memory system
- **Memory pages**: Understanding how the OS manages memory in pages
- **Zero-copy operations**: Understanding how memory mapping can avoid explicit data copying
- **Process memory layout**: Understanding stack, heap, and mapped memory regions

### Low-Level Optimizations

- **Integer arithmetic**: Using integers instead of floats for faster computation (storing temperatures as integers multiplied by 10)
- **Bitwise operations**: Using bitwise XOR and AND for min/max operations to avoid branches
- **Branch elimination**: Understanding how avoiding branches improves CPU pipeline efficiency
- **Pointer arithmetic**: Efficiently advancing through data using pointer arithmetic
- **Explicit length operations**: Using `memcmp`/`memcpy` with explicit lengths instead of null-terminated string functions
- **Inlining**: Understanding how function inlining eliminates call overhead
- **Power-of-two operations**: Using bitwise AND (`&`) instead of modulo (`%`) for power-of-two sizes
- **Character arithmetic**: Converting ASCII characters to integers by subtracting `'0'`
- **Precomputation**: Calculating values (like `city_len`) once and reusing them

### Problem-Solving and Debugging

- **Incremental development**: Building from a simple, working solution and optimizing step by step
- **Performance measurement**: Measuring before and after each optimization to quantify improvements
- **Edge case handling**: Dealing with partial lines at buffer boundaries, handling file boundaries in parallel processing
- **Error handling**: Checking return values (`NULL` from `fopen()`, `MAP_FAILED` from `mmap()`, etc.)
- **Memory safety**: Understanding buffer overflows, null pointer dereferences, and proper memory management

### Software Engineering Practices

- **Code organization**: Structuring code with clear functions and data structures
- **Code readability vs. performance**: Understanding when to prioritize readability and when to optimize
- **Documentation**: Writing clear explanations of complex code sections
- **Iterative improvement**: Refining code through multiple versions, learning from each iteration

This project has been an incredible learning journey, taking me from a C beginner to someone who understands low-level performance optimization, parallel programming, and how computers actually work under the hood. The 200x speedup wasn't just about writing faster code—it was about understanding every layer of abstraction and making informed decisions about when to work closer to the machine.

## References

- [The One Billion Row Challenge (1BRC)](https://github.com/gunnarmorling/1brc) - Original challenge repository by Gunnar Morling
- [One Billion Row Challenge In C](https://www.dannyvankooten.com/blog/2024/1brc/) - Danny van Kooten's C implementation and optimization journey


<link rel="icon" type="image/png" href="../../assets/img/favicons/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/svg+xml" href="../../assets/img/favicons/favicon.svg" />
<link rel="shortcut icon" href="../../assets/img/favicons/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="../../assets/img/favicons/apple-touch-icon.png" />

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
